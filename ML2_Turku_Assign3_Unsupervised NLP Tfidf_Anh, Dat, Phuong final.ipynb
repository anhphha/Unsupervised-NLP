{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts in folder awards_2002: 9923\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_fnames():\n",
    "    \"\"\"Read all text files in a folder.\n",
    "    \"\"\"\n",
    "    fnames = []\n",
    "    for root,_,files in os.walk(\"./abstracts/awards_2002\"):\n",
    "        for fname in files:\n",
    "            if fname[-4:] == \".txt\":\n",
    "                fnames.append(os.path.join(root, fname))\n",
    "    return fnames\n",
    "\n",
    "print(\"Number of abstracts in folder awards_2002: {}\".format(len(get_fnames())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = get_fnames()\n",
    "\n",
    "def read_file(fname):\n",
    "    with open(fname, 'r',encoding=\"ISO-8859-1\") as f:\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                break\n",
    "\n",
    "        # get abstract as a single string\n",
    "        abstract = ' '.join([line[:-1].strip() for line in f])\n",
    "        abstract = re.sub(' +', ' ', abstract)  # remove double spaces\n",
    "        return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i in name_list:\n",
    "    documents.append(read_file(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This proposal focuses on problems in q-series and partitions. There are five separate parts of this work. The first part considers research tied to applications of the construction of representations of Lie algebras. Next the investigator looks at new q-series methods related to special problems in number theory. The third part discusses applications of the Omega software package (http://www.uni-linz.ac.at/research/ combinat/risc/software/Omega/) which is being developed by the investigator in collaboration with colleagues at Linz. The focus in this latter section is on mutli-dimensional partitions. The fourth section is devoted to the study of Bailey chains and a consideration ofhow recent discoveries of the investigator may lead to new applications of this concept. The proposal concludes with consideration of three major unsolved problems in the theory of partitions: (1) the Friedman-Joichi- Stanton conjecture, (2) the Borwein conjecture and (3) the Okada conjecture. Each of these three conjectures has been around for some time. The theme of this proposal put succinctly might be: Building bridges from partitions and q-series (two intrinsically deep and charming but sometimes rather introverted topics) to several branches of mathematics and science. The first two sections are devoted to relating this work to representation theory and number theory, two branches of mathematics; in each instance, it is clear that this interaction will not only enrich the object fields, but also will provide new insights for partitions and q-series. The work on the Omega package has great potential. Here the investigator and his collaborators have found numerous instances where research discoveries have gone from being unthinkable to easily reached. The possible applications to multi-dimensional partitions should lead to insights in combinatorics and, hopefully, the combinatorial aspects of physics. The work on Bailey chains in the past has had profound impact on statistical mechanics in physics. The more this method is advanced, the more we may expect these mutually beneficial applications to continue. The final section on three unsolved problems appears, at first, to be a purely internal study. However, as has often happend in the past, whenever new methods are discovered to solve really hard problems, there is almost always a spillover into vital applications. \n"
     ]
    }
   ],
   "source": [
    "print(documents[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction by tf-idf with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9923"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 53816)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 27001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer1 = TfidfVectorizer(stop_words = 'english', lowercase= True, ngram_range = (1,1), min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix1 = tfidf_vectorizer1.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix1.toarray().shape # N_docs x N_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 190731)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer2 = TfidfVectorizer(stop_words = 'english', lowercase= True, ngram_range = (2,2) ,min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix2.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 117124)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer3 = TfidfVectorizer(stop_words = 'english', lowercase= True, ngram_range = (3,3) ,min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix3 = tfidf_vectorizer3.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix3.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 334856)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer4 = TfidfVectorizer(stop_words = 'english', lowercase= True, ngram_range = (1,3), min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix4 = tfidf_vectorizer4.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix4.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 54049)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters and initialize\n",
    "tfidf_vectorizer5 = TfidfVectorizer(stop_words = 'english', lowercase= True, ngram_range = (1,3), min_df=5, use_idf=True, sublinear_tf=True, max_df=1.0)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix5 = tfidf_vectorizer5.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix5.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6642\tresearch\n",
      "5401\tproject\n",
      "3958\tnew\n",
      "3314\tstudy\n",
      "3184\tprovide\n",
      "3037\tused\n",
      "2967\tdevelopment\n",
      "2944\tstudents\n",
      "2909\tdata\n",
      "2898\tuse\n",
      "2889\tuniversity\n",
      "2868\tunderstanding\n",
      "2787\tusing\n",
      "2774\twork\n",
      "2727\tbased\n",
      "2627\thigh\n",
      "2606\tprogram\n",
      "2596\tdevelop\n",
      "2534\timportant\n",
      "2506\tsystems\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer1.inverse_transform(tfidf_matrix1)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, key words by TF-IDF\n",
      "0.30\tchow\n",
      "0.20\thodge\n",
      "0.20\talgebraic\n",
      "0.15\tgeometry\n",
      "0.15\tsubgroup\n",
      "\n",
      "Document 1, key words by TF-IDF\n",
      "0.24\tfriendships\n",
      "0.23\tcultivating\n",
      "0.22\tethnically\n",
      "0.21\tcontrol\n",
      "0.20\texchanging\n",
      "\n",
      "Document 2, key words by TF-IDF\n",
      "0.31\tupdating\n",
      "0.20\treference\n",
      "0.19\tsecondly\n",
      "0.18\tmethod\n",
      "0.16\tinman\n",
      "\n",
      "Document 3, key words by TF-IDF\n",
      "0.24\tconference\n",
      "0.21\tcomputations\n",
      "0.20\tlearn\n",
      "0.19\ttheory\n",
      "0.19\tgroup\n",
      "\n",
      "Document 4, key words by TF-IDF\n",
      "0.22\tuncontrollable\n",
      "0.21\tcommonality\n",
      "0.19\tuncertainties\n",
      "0.19\tpreferences\n",
      "0.18\talternative\n"
     ]
    }
   ],
   "source": [
    "## Inspect key words per document\n",
    "\n",
    "features1 = tfidf_vectorizer1.get_feature_names()\n",
    "\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, key words by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features1,tfidf_matrix1.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, key words by TF-IDF\n",
      "0.21\talgebraic geometry\n",
      "0.17\tgeometry algebraic\n",
      "0.11\tabelian varieties\n",
      "0.11\talgebraic finite\n",
      "0.11\tchow groups\n",
      "\n",
      "Document 1, key words by TF-IDF\n",
      "0.29\tcontrol engineering\n",
      "0.19\t2002 american\n",
      "0.19\tacademic positions\n",
      "0.19\tspecial sessions\n",
      "0.19\tsupport american\n",
      "\n",
      "Document 2, key words by TF-IDF\n",
      "0.20\tbasis method\n",
      "0.20\tmodel updating\n",
      "0.17\tbased simulation\n",
      "0.16\tproposed effort\n",
      "0.14\tmodel based\n",
      "\n",
      "Document 3, key words by TF-IDF\n",
      "0.19\tgroup theory\n",
      "0.16\topen problems\n",
      "0.13\t28 2003\n",
      "0.13\tarea learn\n",
      "0.13\tconference goal\n",
      "\n",
      "Document 4, key words by TF-IDF\n",
      "0.23\tdesign alternative\n",
      "0.20\tdesign selection\n",
      "0.19\tengineering design\n",
      "0.18\tproduct line\n",
      "0.12\talternative set\n"
     ]
    }
   ],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features2 = tfidf_vectorizer2.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, key words by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features2,tfidf_matrix2.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, key words by TF-IDF\n",
      "0.27\texample algebraic geometry\n",
      "0.27\tfocuses different aspects\n",
      "0.27\tphenomena mathematical problems\n",
      "0.27\tproblems various fields\n",
      "0.27\tquantum mechanics theory\n",
      "\n",
      "Document 1, key words by TF-IDF\n",
      "0.47\tnsf career program\n",
      "0.47\tpreparing students professional\n",
      "0.45\tamerican control conference\n",
      "0.43\tdiverse group students\n",
      "0.41\tsupport travel expenses\n",
      "\n",
      "Document 2, key words by TF-IDF\n",
      "0.44\tmodel based simulation\n",
      "0.28\taim research develop\n",
      "0.28\tdaniel inman virginia\n",
      "0.28\tproposed effort improve\n",
      "0.28\tresearch develop improved\n",
      "\n",
      "Document 3, key words by TF-IDF\n",
      "0.25\t24 28 2003\n",
      "0.25\tgoal provide forum\n",
      "0.25\tincluding solid state\n",
      "0.25\tmarch 24 28\n",
      "0.25\trecent research developments\n",
      "\n",
      "Document 4, key words by TF-IDF\n",
      "0.41\tdevelop integrated framework\n",
      "0.38\tresearch advance state\n",
      "0.38\tresearch develop integrated\n",
      "0.36\tresearch provide opportunities\n",
      "0.35\tprovide opportunities students\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features3 = tfidf_vectorizer3.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, key words by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features3,tfidf_matrix3.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, key words by TF-IDF\n",
      "0.21\tchow\n",
      "0.14\thodge\n",
      "0.14\talgebraic\n",
      "0.14\talgebraic geometry\n",
      "0.11\tgeometry algebraic\n",
      "\n",
      "Document 1, key words by TF-IDF\n",
      "0.22\tcontrol engineering\n",
      "0.15\t2002 american\n",
      "0.15\tacademic positions\n",
      "0.15\tnsf career program\n",
      "0.15\tpreparing students professional\n",
      "\n",
      "Document 2, key words by TF-IDF\n",
      "0.18\tupdating\n",
      "0.15\tbasis method\n",
      "0.15\tmodel updating\n",
      "0.14\tmodel based simulation\n",
      "0.13\tbased simulation\n",
      "\n",
      "Document 3, key words by TF-IDF\n",
      "0.15\tgroup theory\n",
      "0.13\topen problems\n",
      "0.12\tconference\n",
      "0.10\tcomputations\n",
      "0.10\tlearn\n",
      "\n",
      "Document 4, key words by TF-IDF\n",
      "0.17\tdesign alternative\n",
      "0.15\tdesign selection\n",
      "0.14\tengineering design\n",
      "0.14\tuncontrollable\n",
      "0.14\tproduct line\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features4 = tfidf_vectorizer4.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, key words by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features4,tfidf_matrix4.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, key words by TF-IDF\n",
      "0.27\tchow\n",
      "0.18\thodge\n",
      "0.18\talgebraic\n",
      "0.18\talgebraic geometry\n",
      "0.14\tgeometry algebraic\n",
      "\n",
      "Document 1, key words by TF-IDF\n",
      "0.27\tcontrol engineering\n",
      "0.17\tdiverse group students\n",
      "0.17\tparticular students\n",
      "0.17\tstudents seeking\n",
      "0.16\tcultivating\n",
      "\n",
      "Document 2, key words by TF-IDF\n",
      "0.22\tupdating\n",
      "0.18\tmodel based simulation\n",
      "0.16\tbased simulation\n",
      "0.14\treference\n",
      "0.14\tproposed effort\n",
      "\n",
      "Document 3, key words by TF-IDF\n",
      "0.20\tgroup theory\n",
      "0.17\topen problems\n",
      "0.16\tconference\n",
      "0.14\tcomputations\n",
      "0.13\tlearn\n",
      "\n",
      "Document 4, key words by TF-IDF\n",
      "0.21\tdesign alternative\n",
      "0.18\tengineering design\n",
      "0.17\tproduct line\n",
      "0.16\tcommonality\n",
      "0.15\tuncertainties\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features5 = tfidf_vectorizer5.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, key words by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features5,tfidf_matrix5.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km1 = KMeans()\n",
    "km1.fit(tfidf_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features1))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (2424 docs)\n",
      "wireless, software, networks, power, sensor, grid, language, network, distributed, decision\n",
      "\n",
      "Cluster: 1 (1699 docs)\n",
      "polymer, magnetic, spin, quantum, nmr, reactions, laser, nano, molecules, films\n",
      "\n",
      "Cluster: 2 (225 docs)\n",
      "available, zygotic, zygomycota, zygomycetes, zygmund, zworski, zurich, zuni, zro2, zr\n",
      "\n",
      "Cluster: 3 (1802 docs)\n",
      "workshop, conference, teachers, reu, stem, mathematics, engineering, meeting, learning, curriculum\n",
      "\n",
      "Cluster: 4 (1865 docs)\n",
      "ice, ocean, mantle, climate, solar, arctic, galaxies, seismic, stars, wind\n",
      "\n",
      "Cluster: 5 (84 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, informatics, training, microbial, minority, fellowships, biology\n",
      "\n",
      "Cluster: 6 (578 docs)\n",
      "equations, manifolds, spaces, algebraic, theory, geometry, algebras, quantum, hyperbolic, topology\n",
      "\n",
      "Cluster: 7 (1246 docs)\n",
      "genes, species, protein, plant, proteins, gene, plants, genome, evolutionary, genetic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix1, km1.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "\n",
    "km1 = KMeans()\n",
    "km1.fit(tfidf_matrix2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features2))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (377 docs)\n",
      "physical chemistry, transition metal, surface chemistry, number areas, areas including, ray diffractometer, time resolved, focus research, analytical surface, reactive intermediates\n",
      "\n",
      "Cluster: 1 (57 docs)\n",
      "sciences fellowship, mathematical sciences, biological informatics, goal fellowship, sciences fellowships, research training, entitled modeling, entitled influence, entitled role, fellowship fy2002\n",
      "\n",
      "Cluster: 2 (758 docs)\n",
      "mathematics science, middle school, computer science, community college, engineering technology, science mathematics, program intended, graduate education, school districts, math science\n",
      "\n",
      "Cluster: 3 (386 docs)\n",
      "earth science, researchers represent, disciplinary databases, data sets, proof concept, knowledge discovery, digital library, analysis networked, amounts diverse, agencies geological\n",
      "\n",
      "Cluster: 4 (6352 docs)\n",
      "postdoctoral fellowship, number theory, real time, algebraic geometry, dynamical systems, representation theory, microbial biology, solar wind, dark matter, star formation\n",
      "\n",
      "Cluster: 5 (765 docs)\n",
      "reu site, research experience, spin polarized, self assembly, shipboard scientific, scientific support, support equipment, equipment including, magnetic properties, condensed matter\n",
      "\n",
      "Cluster: 6 (746 docs)\n",
      "sea ice, ice sheet, ice core, organic matter, carbon cycle, 000 years, carbon dioxide, climate change, southern ocean, ice cores\n",
      "\n",
      "Cluster: 7 (482 docs)\n",
      "research fellowship, ii project, phase ii, fuel cell, drug delivery, fuel cells, silicon carbide, high temperature, rich media, cutting tool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix2, km1.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km1 = KMeans()\n",
    "km1.fit(tfidf_matrix3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features3))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (100 docs)\n",
      "support month research, award support month, fellowship program enables, international research fellowship, month research fellowship, archaeal bacterial viral, additional training microbial, bacterial viral species, avail unique research, assist new scientists\n",
      "\n",
      "Cluster: 1 (142 docs)\n",
      "engineering initiative nsf, funded experimental physical, initiative nsf 01, 157 category ner, 01 157 category, project funded experimental, physical chemistry program, students postdoctoral research, project addresses fundamental, received response nanoscale\n",
      "\n",
      "Cluster: 2 (16 docs)\n",
      "astrophysics postdoctoral fellowship, astronomy astrophysics postdoctoral, awarded nsf astronomy, carry program research, fellowship carry program, research education experimental, particle physics based, experimental elementary particle, education experimental elementary, requests support group\n",
      "\n",
      "Cluster: 3 (92 docs)\n",
      "carnegie mellon university, collaborative research project, basis equality reciprocity, international collaborative research, supports collaborative research, award supports collaborative, bringing leading experts, project multidisciplinary international, multidisciplinary international collaborative, principal investigator dr\n",
      "\n",
      "Cluster: 4 (9124 docs)\n",
      "mathematical sciences fellowship, national science foundation, partial differential equations, science foundation support, small grant exploratory, grant exploratory research, proposal number cts, foundation support dr, dissertation research project, world wide web\n",
      "\n",
      "Cluster: 5 (32 docs)\n",
      "doctoral dissertation research, award award provide, enable promising student, establish strong independent, economic cultural environmental, dissertation research improvement, global innovation networks, serve baseline comparison, tree ring chronologies, depth interviews key\n",
      "\n",
      "Cluster: 6 (386 docs)\n",
      "business innovation research, innovation research sbir, small business innovation, sbir phase project, research sbir phase, innovation research phase, research phase project, phase ii project, phase project develop, phase project proposes\n",
      "\n",
      "Cluster: 7 (31 docs)\n",
      "biological informatics occupational, informatics fy2002 fellowship, biological informatics fy2002, biological informatics research, biology informational computational, career biological informatics, computational mathematical statistical, contributing future vitality, attain goal fellowship, academia industry attain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix3, km1.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km1 = KMeans()\n",
    "km1.fit(tfidf_matrix4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features4))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (446 docs)\n",
      "fuel, phase ii project, sensor, drug, phase ii, ii project, coatings, fuel cell, silicon, drug delivery\n",
      "\n",
      "Cluster: 1 (1721 docs)\n",
      "sensor, wireless, grid, routing, software, agents, mobile, power, code, memory\n",
      "\n",
      "Cluster: 2 (578 docs)\n",
      "manifolds, equations, algebraic, algebras, hyperbolic, spaces, number theory, representation theory, conjecture, string\n",
      "\n",
      "Cluster: 3 (225 docs)\n",
      "available, zygotic, zygomycota aftol project, zygomycota aftol, zygomycota, zygomycetes ascomycetes basidiomycetes, zygomycetes ascomycetes, zygomycetes, zygmund operators spaces, zygmund operators\n",
      "\n",
      "Cluster: 4 (126 docs)\n",
      "fellowship, sciences fellowship, mathematical sciences fellowship, mathematical sciences, mathematical, sciences, postdoctoral fellowship, biological informatics, postdoctoral, informatics\n",
      "\n",
      "Cluster: 5 (1966 docs)\n",
      "conference, workshop, stem, symposium, teachers, reu, reu site, meeting, girls, scholarship\n",
      "\n",
      "Cluster: 6 (3540 docs)\n",
      "ice, contract, galaxies, arctic, mantle, stars, forest, solar, soil, co2\n",
      "\n",
      "Cluster: 7 (1321 docs)\n",
      "nmr, spin, polymer, magnetic, french, films, reactions, cnrs, nanoparticles, nanotubes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix4, km1.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km1 = KMeans()\n",
    "km1.fit(tfidf_matrix5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features5))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (1232 docs)\n",
      "stem, reu, teachers, reu site, scholarship, mathematics, girls, teacher, csems, fellows\n",
      "\n",
      "Cluster: 1 (3028 docs)\n",
      "ice, mantle, solar, arctic, galaxies, ocean, forest, stars, wind, species\n",
      "\n",
      "Cluster: 2 (444 docs)\n",
      "phase ii, fuel, phase ii project, ii project, sensor, coatings, membrane, drug, optical, fuel cell\n",
      "\n",
      "Cluster: 3 (2299 docs)\n",
      "manifolds, equations, quantum, grid, spaces, wireless, algebraic, code, software, power\n",
      "\n",
      "Cluster: 4 (225 docs)\n",
      "available, zurich, zr, zooplankton species, zooplankton, zoology, zoological, zoning, zones long lived, zones long\n",
      "\n",
      "Cluster: 5 (1802 docs)\n",
      "protein, spin, nmr, proteins, complexes, polymer, molecules, magnetic, reactions, arabidopsis\n",
      "\n",
      "Cluster: 6 (126 docs)\n",
      "fellowship, sciences fellowship, mathematical sciences fellowship, mathematical sciences, mathematical, sciences, postdoctoral fellowship, abroad, biological informatics, postdoctoral\n",
      "\n",
      "Cluster: 7 (767 docs)\n",
      "workshop, conference, symposium, meeting, french, cnrs, gordon, congress, sessions, speakers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix5, km1.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###change k for ngram =(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k1=3\n",
    "model = KMeans(n_clusters= k1)\n",
    "model.fit(tfidf_matrix5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features5))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (225 docs)\n",
      "available, zurich, zr, zooplankton species, zooplankton, zoology, zoological, zoning, zones long lived, zones long\n",
      "\n",
      "Cluster: 1 (7098 docs)\n",
      "ice, mantle, quantum, magnetic, protein, solar, manifolds, arctic, sensor, fault\n",
      "\n",
      "Cluster: 2 (2600 docs)\n",
      "workshop, conference, fellowship, sciences fellowship, mathematical sciences fellowship, mathematical sciences, stem, reu, teachers, mathematical\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(tfidf_matrix5, model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
